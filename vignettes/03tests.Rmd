---
title: "unitizeR - Test Details"
author: "Brodie Gaslam"
date: "`r Sys.Date()`"
output:
    rmarkdown::html_vignette:
        toc: true
        css: styles.css

vignette: >
  %\VignetteIndexEntry{03 - Unitizer Tests}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

## Test Details

### Test Outcomes

So far we have only seen three test outcomes: New, Passed, and Failed.  For reference here is a list of all the test outcomes:

* New: the test is not present in the `unitizer` store and needs to be reviewed prior to storage to ensure it actually does what you expect
* Passed: the test passed successfully and you will not be prompted to review
* Failed: the stored test differs from the one produced by the re-evaluation of the same expression, as was the case in our examples above
* Deleted/Removed: a test present in the `unitizer` store no longer exists in the test R file
* Corrupted/Error: an error occurred while attempting to compare the new and reference tests; this should occur very rarely and is likely the result of using a custom comparison function to compare the tests (see `unitizer_sect` section for more details on custom comparison functions).  When this happens `unitizer` has no way of knowing whether the test passed or failed; you can think of it as an `NA` outcome.

When reviewing tests, `unitizer` will group tests by test type, so you will review all new tests  in one go, then the failed tests, and so on.  The implication here is that the order that you review tests may not be the same as the order they appear in in the test file.

### What Constitutes a Test?

As noted previously simple assignments are not considered tests.  They are stored in the `unitizer` store, but you are not asked to review them, and their values are not be compared to values produced by future evaluations of the same assignment expression.  The implicit assumption is that if there is an assignment the intent is to use it as an input to another test that will be reviewed, so skipping assignment review saves some unnecessary user interaction.

You can force assignments to become tests by wrapping them in parentheses:
```
a <- my_fun(25)     # this is not a test
(a <- my_fun(42))   # this is a test
```
In addition to assignments, some expressions are not considered tests.  At this time, another expression that is ignored is `library(...)`, as the return value for that function varies depending on the currently loaded libraries.

### `unitizer` Test Components

The following aspects of a unitizer tests are recorded for future comparison:

* Value (whether invisible or not)
* Conditions
* Screen (stdout) output
* Message (stderr) output
* Whether the expression issued an "abort" `invokeRestart`

Currently only the first two elements are actually compared when determining whether a test passes or fails.  These two should capture almost all you would care about from a unit test perspective.

Screen output is omitted from comparison because it can be caused to vary substantially by factors unrelated to source code changes (e.g. console display width).  Message output is omitted because all typical mechanisms for producing stderr output also produce conditions with messages embedded, so it is usually superfluous to compare them.  The "abort" `invokeRestart` is omitted because it generally is implied by the presence of an error condition and actively monitoring it clutters the diagnostic messaging produced by `unitizer`.

While we omit the last three components from comparison, this is just default behavior.  You can change this by using the `compare` argument for `unitizer_sect` (see the `unitizer_sect` section below).

## Test Sections

### `untizer_sect`

Often it is useful to group tests in sections for the sake of documentation and clarity.  Here is a slghtly modified version of the original demo file with sections:

```
library(unitizer.fastlm)

unitizer_sect("Create Fastlm Object", {
  x <- 1:100
  y <- x ^ 2
  res <- fastlm(x, y)
  res
})
unitizer_sect("Accessor Functions", {
  get_slope(res)
  get_rsq(res)
  get_intercept(res)
})
unitizer_sect("Error Handling", {
  fastlm(x, head(y))
  get_rsq("cabbage")
})
```
Now re-running `unitizer` segments everything by section:
```
unitize("inst/example.pkgs/fastlm.2/tests/unitizer/fastlm.R")
+-----------------------------------------------------------------------------+
| unitizer for: inst/example.pkgs/fastlm.2/tests/unitizer/fastlm.R            |
+-----------------------------------------------------------------------------+

                      Pass  Fail   New
Create Fastlm Object     0     0     1
Accessor Functions       0     0     3
Error Handling           0     0     2
**Total**                0     0     6
```
If there are tests that require reviewing, each section will be reviewed in turn.

Note that `unitizer_sect` does not create separate evaluation environments for each section.  Any created object will be available to all lexically subsequent tests, regardless of whether they are in the same section or not.

It is possible to have nested sections, though at this point in time `unitizer` only explicitly reports information at the outermost section level.

### Controlling Test Comparison

By default tested components (values and conditions) are compared with `all.equal`.  If you want to override the function used for value comparisons it is as simple as creating a new section for the tests you want to compare differently and use the `compare` argument:

```
unitizer_sect("Accessor Functions", compare=identical,
  {
    get_slope(res)
    get_rsq(res)
    get_intercept(res)
} )
```
The values produced by these three tests will be compared using `identical` instead of `all.equal`.  If you want to modify how other components of the test are compared, then you can pass a `unitizerItemTestsFuns` object as the value to the `compare` argument instead of a function:
```
unitizer_sect("Accessor Functions",
  compare=unitizerItemTestsFuns(
    value=identical,
    output=all.equal,
    message=identical
  ),
  {
    get_slope(res)
    get_rsq(res)
    get_intercept(res)
} )
```
This will cause the value of tests to be compared with `identical`, the screen output with `all.equal`, and messages (stderr) with `identical`.

If you want to change the comparison function for conditions, keep in mind that what you are comparing are `conditionList` objects so it isn't completely straightforward to compare them (see `getMethod("all.equal", "conditionList")`).  In the future we might expose a better interface for custom comparison functions for conditions (see issue #32).

If you need to have different comparison functions within a section, use nested sections.  While `unitizer` will only report the outermost section metrics in top-level summaries, the specified comparison functions will be used for each nested section.

## Nitty Gritty

### Matching Tests

Whenever you re-run `unitizer` on a file that has already been "unitized", `unitizer` matches the expressions in that file to what the expressions were in the file when it was first run.  `unitizer` matches only on the deparsed expression, and does not care at all where in the file the expression occurs.  If multiple identical expressions exist in a file they will be matched in the order they show up.

One particularly important point is that the `unitizer_sect` in which a test was when it was first "unitized" has no bearing whatsoever on matching a new test to a reference test.  For example, if a particular test was in "Section A" when it was first unitized, but in the current version of the test file it is in "Section X", that test will be matched to the current one in "Section X".

### Commenting Tests

`unitizer` parses the comments in the test files and attaches them to the test that they document.  Comments are attached to tests if they are on the same line as the test, or just above the test.  Comments are displayed with the test expression during the interactive review mode.

### Options and Streams

In order to properly capture output, this function must mess with streams and options.  In particular, it will do the following:

* temporarily set `options(warn=1L)` during expression evaluation
* temporarily set `options(error=null)` during expression evaluation
* use `sink()` to capture any output to `stdout`
* use `sink(type="message")` to capture output to `stderr`

This should all be transparent to the user, unless the user is also attempting to modify these settings in the test expressions.  The problematic interaction are around the `options` function.  If the user sets `options(warn=1)` with the hopes that setting will persist beyond the execution of the test scripts, that will not happen.  If the user sets `options(error=recover)` or some such in a test expression, and that expression throws an error, you will be thrown into recovery mode with no visibility of `stderr` or `stdout`, which will make for pretty challenging debugging.

If the function is run with message diversion already activated, then it will not capture any messages produced by the test expressions.  If one of the test expressions enables message capture, then this function will stop capturing messages from that point on.  If after this another test expression disables message capture, this function will start capturing messages starting with the subsequent test expression (i.e. any message output between the expression self disabling message output and the end of the expression will go to `stdout`).

### Error Handling

Because `unitize` evaluates test expressions within a call to `withCallingHandlers`, there are some limitations on successfully running `unitize` inside your own error handling calls.  In particular, `unitize` will not work properly if run inside a `tryCatch` or `try` statement. If test expressions throw conditions, the internal `withCallingHandlers` will automatically hand over control to your `tryCatch`/`try` statement without an opportunity to complete `unitize` computations.  Unfortunately there does not seem to be a way around this since we have to use `withCallingHandlers` so that test statements after non-aborting conditions are run.
