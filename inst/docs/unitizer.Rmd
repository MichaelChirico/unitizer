# unitizeR - Turn Arbitrary Code into Unit Tests

## Introduction
----

### Storing Expressions and Their Results as Tests

One of the more important purposes of unit tests is to ensure that previously tested functionality does not break as a result of changes to the code base.  `unitizer` seeks to simplify this aspect of unit testing by taking advantage of the following:

* code functionality is demonstrated by evaluating expressions and observing results
* unit tests verify that specific expressions continue to produce the same results
* in R, expressions are objects, results are objects, and both can be stored

`unitizer` evaluates arbitrary expressions and stores them along with the result of their evaluation.  Re-running the same `unitizer` then compares the new results for each expression to the previously stored results.  If there are any differences `unitizer` breaks out into an interactive mode to allow comparison of the old evaluation to the new one, and if necessary, update the tests.

### But Why?

There are some note-worthy benefits to this approach, in particular:

 1. tests for expressions that produce complex objects (e.g. `lm(y ~ x + z), DF)`) are trivial to write because the result of the expression becomes the unit test
 2. there is no coding overhead to writing the tests; what you typed in the console when you were developing functionality can be used as the unit tests (taken to the extreme, you could just use the history file from when you were informally testing functionality)
 3. (almost) all aspects of the evaluation can be captured and used as part of the tests; for example, any conditions signalled during evaluation will be captured for comparison to future evaluations

I first embarked on this project to address point 1.  In the absence of `unitizer`, testing that complex objects are created as expected requires running the test expression, deparsing the result, and then copy/pasting the ungainly string into a file with an `all.equal` or equivalent such statement.  While this isn't back-breaking, it does start taking on sisyphean overtones when you set out to build comprehensive tests, or god forbid, realize you need to slightly modify the output of your functions.

Points 2. and 3. are gravy.  For example, for 3., you can easily test that your code both fails, and that when it does, it produces the desired error message.

One aspect of evaluation that isn't currently captured but hopefully will be in the future are side effects such as plots.

### How Do I Use It?

There are several ways to use `unitizer`, but the simplest one is to create a file with your test expressions.  For example, suppose we develop a `fastlm` function that does simple single variable regression to calculate slope, intercept, and coefficient.  Our test file might look like so:

    ## file: tests/unitizer/fastlm.unitizer.R

    library(fastlm)

    x <- 1:100
    y <- ((1:100) / 10) ^ 2

    (res <- fastlm(x, y))   # parens to force treatment as test
    get_slope(res)
    get_intercept(res)
    get_rsq(res)

    # Cause errors

    fastlm(x, head(y))
    fastlm(x, NULL)

    get_slope("hello")
    get_rsq(FALSE)

Then, we just run:

    unitize("tests/unitizer/fastlm.unitizer.R")

The expressions will be evaluated, and results, errors, warnings, screen ouput, etc. will be stored.  If this is the first time you've run `unitize`, you will be prompted to review each test in an interactive environment to verify the expressions actually produce the desired outcome.  Once that's done, everything is stored in an `RDS` and voila, unit tests! 

Later on, you can just re-run:

    unitize("tests/unitizer/fastlm.unitizer.R")

If everything is working as before then all the tests will pass and you can keep coding merrily.  If you introduced regressions, then `unitizer` will highlight the failing tests and allow you to review them interactively.  If you added more tests to the file since the last time you ran `unitize`, those tests will be added to the RDS subject to your review.

### Learning To Use `unitizer`

You don't need to go through the entirity of this oxymoronically lengthy vignette to start using `unitizer`.  The `unitizer` interactive environment strives to be self explanatory, and provides contextual help.  You can always type `H` at the `unitizer` prompt to recieve assitance.

## A Detailed Run-through
----

### First Run - Store Tests

The first time we run our tests we are prompted to review them one by one to confirm they evaluated to what we expect.  This is what happens when we run `unitize("tests/unitizer/lm.R")`:

    > unitize("tests/unitizer/fastlm.unitizer.R")
    +--------------------------------------------------------------------+
    | unitizer for: tests/unitizer/fastlm.unitizer.R                     |
    +--------------------------------------------------------------------+

        Pass     Fail      New  Deleted                                   
           0        0        8        0 

    - New ----------------------------------------------------------------

    Test script contains tests not present in unitizer. Add new item to 
    store ([Y]es, [N]o, [B]ack, [R]eview, [Q]uit, [H]elp)?

    > library(fastlm)
    > x <- 1:100
    > y <- ((1:100)/10)^2
    # parens to force treatment as test
    > (res <- fastlm(x, y))
     intercept      slope        rsq 
    -17.170000   1.010000   0.938679 
    attr(,"class")
    [1] "fastlm"
    unitizer> 

This is telling us that there are eight new tests in our file.  The "new" banner line is telling us we are about to review the new tests (the only type in this case), and the text that follows provides options to type at the `unitizer>` prompt.

At this point, `unitizer` starts showing you the results of evaluating the tests.  Note that assignment expressions are not considered tests; they are stored, but you are not asked to review them, and their values will not be compared to values produced by future evaluations.  The implicit assumption is that if there is an assignment the intent is to use it as an input to another test that will be reviewed, so skipping assignment review saves some unnecessary user interaction.

You can force assignments to become tests by wrapping them in parentheses, which is why the `unitizer>` prompt appears after the `(res <- fastlm(x, y))`.  As we noted earlier, the prompt allows you to run R expressions, so we can use this opportunity to check that our function did what it is supposed to:

    unitizer> lm(y ~ x, data.frame(x, y))$coefficients
    (Intercept)           x 
         -17.17        1.01 
    unitizer>

This matches our computation, so we will accept the test by typing in `Y` and hitting ENTER:

    unitizer> Y
    > get_slope(res)
    [1] 1.01
    unitizer>

The first test, `(res <- fastlm(x, y))` is now queued for storing, and we're prompted about the second test.  We will also accept this one, as well as the next 3, which brings us to:

    # Cause errors
    > fastlm(x, head(y))
    Error in data.frame(x = x, y = y) : 
      arguments imply differing number of rows: 100, 6
    unitizer> 

This test caused an error.  It was supposed to as the inputs were purposefully wrong.  We want to store this particular error to ensure that in the future our function keeps erroring out in the same way with this type of erroneous input.  Conveniently all we have to do is hit `Y`.  The error becomes part of the store and any future re-runs will compare the result of the expression against the stored error and will notify if the expression fails to produce an error or produces a different error.  So will we accept this test, as well as the remaining tests, which brings us to the final prompt:

    unitizer> Y
    = Confirm Changes ====================================================

    You are about to IRREVERSIBLY:
    - Add 8 out of 8 new tests
    unitizer>

So far we have only queued the tests for storage.  By answering `Y` at this prompt we will be storing all the tests we accepted into the unitizer store:

    unitizer> Y
    unitizer updated

That's it.  We're done for now.  On the actual R prompt:

    > list.files("tests/unitizer")
    # [1] "fastlm.unitizer.R"   "fastlm.unitizer.rds"

The `.rds` file was generated by unitizer and contains the data we will use in the future to ensure we have not introduced regressions in our code.

### Second Run -- Check For Regressions

Turns out our `fastlm` wasn't fast at all.  It was just a wrapper for the standard `lm`.  But that was just a starting point.  After our initial run we went ahead and figured out all the formulas needed to calculate the slope, intercept and R squared of a single variable linear regression, and plugged them into our function.  But did we do it right?  Let's check:

    > unitize("tests/unitizer/fastlm.unitizer.R")
    +-------------------------------------------------------------------+
    | unitizer for: tests/unitizer/fastlm.unitizer.R                    |
    +-------------------------------------------------------------------+

        Pass     Fail      New  Deleted                                  
           2        6        0        0 

    - Failed ------------------------------------------------------------

    Reference test does not match new test from test script. Overwrite item in store with new value ([Y]es, [N]o, [B]ack, [R]eview, [Q]uit, [H]elp)?

    # parens to force treatment as test
    > (res <- fastlm(x, y))
    Test Failed Because:
    Value mismatch: Mean relative difference: 18889539312
    unitizer> 

Clearly something is wrong.  Six of our eight tests failed.  `unitizer` prompts us to review the failed tests and decide whether to replace the existing tests with the new ones.  The first failed test is the one that calculates the values related to the linear regression.  `unitizer` provides us with two special objects that we can use to examine the reason for our test failure: `.new` and `.ref`, each representing the value produced by the test we just ran as well as the value produced by the original test we stored.  We can use these to get more details on the nature of the failure:

    unitizer> .new
        intercept         slope           rsq 
    -3.541306e+11  7.012486e+09  9.688545e-01 
    attr(,"class")
    [1] "fastlm"
    unitizer> .ref
     intercept      slope        rsq 
    -17.170000   1.010000   0.938679 
    attr(,"class")
    [1] "fastlm"
    unitizer>  

By comparing the output of `.new` and `.ref` we can see that all our computations are just wrong (note we could also have re-evaluated our test instead of typing `.new` at the prompt, but if the computation is slow using `.new` avoids recomputation, and the only way to get the original value is with `.ref`).

We now know that our new calculations are wrong, so we will reject this test as well as the subsequent related tests (all the `get*` functions that retrieve computations will obviously fail too), which takes us to our error causing tests:

    unitizer> N
    > get_intercept(res)
    Test Failed Because:
    Value mismatch: Mean relative difference: 5.941176
    unitizer> N
    # Cause errors
    > fastlm(x, head(y))
    Error in fastlm(x, head(y)) : 
      Arguments `x` and `y` must be the same length.
    Test Failed Because:
    Conditions mismatch: 
      + There is 1 condition mismatch; showing first mismatch at condition #1
      + Error condition messages do not match
    unitizer>

This is telling us that our tests failed because they did not produce the same error message.  We can check with `getConds` (note `getConds` returns a list of conditions, which is why we subset with `[[1]]` to see the first condition):

    getConds(.new)[[1]]
    <simpleError in fastlm(x, head(y)): Arguments `x` and `y` must be the same
    length.>
    unitizer> getConds(.ref)[[1]]
    <simpleError in data.frame(x = x, y = y): arguments imply differing number 
    of rows: 100, 6>

By using the `.new`/`.ref` objects as an argument we can tell `getConds` which of the condition we wish to view: the newly evaluated ones or the reference ones that were stored earlier.  In this case, while the condition messages changed, the general meaning did not and since we added our own error handler rather than rely on internal error handling this change is reasonable.

The remaining two failed tests also failed for similar reasons, so we will accept them too, which again takes us to the final prompt:

    unitizer> Y
    > fastlm(x, NULL)
    Error in fastlm(x, NULL) : Arguments `x` and `y` must be numeric.
    Test Failed Because:
    Conditions mismatch: 
      + There is 1 condition mismatch; showing first mismatch at condition #1
      + Error condition messages do not match
    unitizer> Y
    = Confirm Changes ===================================================

    You are about to IRREVERSIBLY:
    - Replace 2 out of 6 failed tests
    unitizer> Y
    unitizer updated

Our `unitizer` `.rds` is now updated.  We replaced the two error tests with their new versions, but did not replace the tests related to the actual copmutations since the new computations are wrong.

### Third Run -- Fix Problems

We figured out the problem in our new implementation of `fastlm` and fixed it.  Let's run unitizer one more time:

    > unitize("tests/unitizer/fastlm.unitizer.R")
    +--------------------------------------------------------------------+
    | unitizer for: tests/unitizer/fastlm.unitizer.R                     |
    +--------------------------------------------------------------------+

        Pass     Fail      New  Deleted                                   
           8        0        0        0 

    All tests passed; nothing to store.

Success! Our new test code now validates properly.  We have a functional `fastlm`!

### A Segue on Test Types

So far we have only seen three test types: New, Passed, and Failed.  For reference here is a list of all the test types:

* Passed: the test passed successfully and you will not be prompted to review
* Failed: the stored test differs from the value produced by the re-evaluation of the same expression, as was the case in our examples above
* New: the test is not present in the `unitizer` store and needs to be reviewed prior to storage to ensure it actually does what you expect
* Deleted/Removed: a test present in the `unitizer` store no longer exists in the test R file
* Corrupted/Error: an error occurred while attempting to compare the new and reference tests; this should occur very rarely and is likely the result of using a custom comparison function to compare the tests (see `unitizer_sect` section for more details on custom comparison functions).  When this happens `unitizer` has no way of knowing whether the test passed or failed; you can think of it as an `NA` outcome.

When reviewing tests, `unitizer` will group tests by test type, so you will review all new tests for each `unitizer_sect` in one go, then the failed tests, and so on.

## Details On Tests
----

### Test Sections: `unitizer_sect`

Often it is useful to group tests in sections for the sake of documentation and clarity.  Here we updated our original test file to add some sections:

    ## file: tests/unitizer/fastlm.unitizer.R

    library(fastlm)

    unitizer_sect(
      "Base Tests", 
      {
        x <- 1:100
        y <- ((1:100) / 10) ^ 2

        (res <- fastlm(x, y))   # parens to force treatment as test
        get_slope(res)
        get_intercept(res)
        get_rsq(res)
    } )
    unitizer_sect(
      "Test Error Handling", 
      {
        fastlm(x, head(y))
        fastlm(x, NULL)

        get_slope("hello")
        get_rsq(FALSE)
    } )

Now re-running `unitizer` segments everything by section:

    > unitize("tests/unitizer/fastlm.unitizer.R")
    +-------------------------------------------------------------------+
    | unitizer for: tests/unitizer/fastlm.unitizer.R                    |
    +-------------------------------------------------------------------+

                        Pass     Fail      New  Deleted                  
    Base Tests             4        0        0        0
    Test Error H...        4        0        0        0
    **Total**              8        0        0        0

    All tests passed; nothing to store.

Similarly, if there are tests that require reviewing, each section will be reviewed in turn.

### `unitizer` Test Components

The following aspects of a unitizer tests are recorded for future comparison:

* Value (whether invisible or not)
* Conditions
* Screen output
* Message (stderr) output
* Whether the expression issued an "abort" `invokeRestart`

Currently only the first two elements are actually compared when determining whether a test passes or fails.  These two should capture almost all you would care about from a unit test perspective.  Screen output is omitted from comparison because it can be caused to vary substantially by factors unrelated to source code changes (e.g. console display width).  Message output is omitted because all obvious mechanisms for producing stdout output also produce conditions with messages embedded, so it is usually superfluous to compare them.  The "abort" `invokeRestart` is omitted because it generally is implied by the presence of an error condition and actively monitoring it clutters the diagnostic messaging produced by `unitizer`.

While we omit the last three components from comparison, this is just default behavior.  You can change this by using the `compare` argument for `unitizer_sect` (see next section).

### Controlling Test Comparison

By default tested components (values and conditions) are compared with `all.equal`.  If you want to override the function used for value comparisons it is as simple as creating a new section for the tests you want to compare differently and use the `compare` argument:

     unitizer_sect(
      "Base Tests Strict", compare=identical,
      {
        get_slope(res)
        get_intercept(res)
        get_rsq(res)
    } )

The values produced by these three tests will be compared using `identical` instead of `all.equal`.  If you want to modify how other components of the test are compared, then you can pass a `unitizerItemTestsFuns` object as the value to the `compare` argument instead of a function:

     unitizer_sect(
      "Base Tests Strict", 
      compare=unitizerItemTestsFuns(
        value=identical, 
        output=all.equal,
        message=identical
      ),
      {
        get_slope(res)
        get_intercept(res)
        get_rsq(res)
    } )

This will cause the value of tests to be compared with `identical`, the screen output with `all.equal`, and messages (stdout) with `identical`.  If you want to change the comparison function for conditions, keep in mind that what you are comparing are lists of conditions.  `unitizer` by default uses the `all.equal.condition_list` functions to compare conditions, and you should probably review that function if you intend to use a custom condition comparison function.

### Matching Tests

Whenever you re-run `unitizer` on a file that has already been "unitized", `unitizer` matches the expressions in that file to what the expressions were in the file when it was first run.  `unitizer` matches only on the deparsed expression, and does not care at all where in the file the expression occurs.  If multiple identical expressions exist in a file they will be matched in the order they show up.

One particularly important point is that the `unitizer_sect` in which a test was when it was first "unitized" has no bearing whatsoever on matching.  In other words, the matching is done with no regard for what `unitizer_sect` the new or reference tests are/where in.

### Commenting Tests

`unitizer` parses the comments in the test files and attaches them to the test that they document.  Comments are attached to tests if they are on the same line as the test, or just above the test.  Comments are displayed with the test expression during the interactive review mode.

## The Interactive Environment
----

### Example Set-up

To examine the interactive environment more thoroughly we will go back to the test review when our new version of `fastlm` was returning incorrect values.  This is the fourth failed test:

    > get_rsq(res)
    Test Failed Because:
    Value mismatch: Mean relative difference: 0.03214676
    unitizer>

We will use this test to illustrate features / subtleties of `unitizer` prompt.

### `unitizer` Commands

Much like the `browser()` prompt, the `unitizer` prompt accepts several special expressions that allow you to control `unitizer` behavior.  What the expressions are and what they do depends on context.  We will review them in the context of the failed test described above.  Let's go back and look at what the `unitizer` prompt stated before we started reviewing our failed tests:

    - Failed ------------------------------------------------------------

    Reference test does not match new test from test script. Overwrite 
    item in store with new value ([Y]es, [N]o, [B]ack, [R]eview, [Q]uit, 
    [H]elp)?

This clearly lays out all the special commands available to us:

* `Y` will accept the new value as the correct reference value to use for a test
* `N` will keep the previous reference value as the reference value for future tests
* `B` takes us back to the previously reviewed test (see "Navigation" sub-section)
* `R` allows us to navigate to any previously reviewed test (see "Test Navigation" sub-section)
* `Q` quits `unitizer` (see "Quitting `unitizer`")
* `H` provides contextual help

If you type any of those letters (without backticks) into the `unitizer` prompt you will cause `unitizer` to respond as described above instead of evaluating the expression as it would be at the normal R console prompt.  If you have a variable assigned to one of those letters and you wish to access it, you can do so with any of `get`, `(`, `print`, etc.  For example, suppose we stored something in `Y`, then to access it all these commands would work:

* `(Y)`
* `get("Y")`
* `print(Y)`

`unitizer` checks for an exact match of a user expression to the special command letters, so something like `(Y)` does not match `Y` which allows you to reach the value stored in `Y`.

If at any time you forget what `unitizer` options are available to you you can just hit the "ENTER" key and `unitizer` will re-print the options to screen.

### Test Navigation

The `B` and `R` `unitizer` commands allow you to navigate through previously evaluted tests.  This is useful if you realize that you incorrectly accepted or rejected an earlier test, but don't wish to quit `unitizer` completely and lose all the other properly reviewed tests. `B` just steps you back to the previous test.  `R` gives you the option to go back to any previously reviewed test.  At this time it is not possible to skip ahead to unreviewed tests.

`B` is trivially straightforward, so we will not discuss it further.  We will type `R` at the prompt of our fourth failed test to examine what it does:

    unitizer> R
    What test do you wish to review (input an integer-like number corresponding to a test)?

      2. (res <- fastlm(x, y)) - Failed:N
      3. get_slope(res) -------- Failed:N
      4. get_intercept(res) ---- Failed:N
    unitizer>

`R` produces a list of all the test we have reviewed.  In this case these are all failed tests which we chose not to keep (hence the `Failed:N` appended at the end of each deparsed expression).  If we have second thoughts about our choice we can revisit the test by inputting the test number at the prompt (note that the test numbering is a bit odd; there are reasons for this but they aren't worth discussing).  Suppose we want to review test 3, the second on our list.  Then:

    unitizer> 3
    You are re-reviewing a test; previous selection was: "N"
    > get_slope(res)
    Test Failed Because:
    Value mismatch: Mean relative difference: 6943055624
    unitizer> 

`unitizer` tells us we are re-reviewing this test and that previously we had chosen not to keep the new version.  At this point we could re-examine the test, and potentially change our previous selection.

### `.new` and `.ref`

As we saw in our earlier example there are special objects available at the prompt: `.new`, and for all but new tests, `.ref`.  These objects contain the values produced by the newly evaluated test (`.new`) and by the test when it was previously run and accepted (`.ref`).  `.new` might seem a bit superfluous since the user can always re-evaluate the test expression at the `unitizer` prompt to review the value, but if that evaluation is slow you can save a little time by using `.new`.  `.ref` is the only option you have to see what the test used to produce back when it was first accepted into the `unitizer` store.

`.new` and `.ref` contain the values produced by the tests, but sometimes it is useful to access other aspects of the test evaluation.  To do so you can use the `get*` accessor functions in conjunction with the `.new` or `.ref` objects:

* `getTest(.new)` gets general information about the test
* `getVal(.new)` returns the test value; equivalent to typing `.new` at the prompt
* `getConds(.new)` returns the list of conditions produced by the test
* `getMsg(.new)` returns the stderr captured during test evaluation
* `getOut(.new)` returns the screen output captured during test evaluation
* `getExpr(.new)` returns the test expression (don't confuse this with `getCall`, a built-in R function with no relation to `unitizer`)
* `getAborted(.new)` returns whether the test expression invoked an "abort" restart

You can substitute `.ref` for `.new` in any of the above, provided that `.ref` is defined (i.e. that won't work when you are reviewing new tests since there is no corresponding reference test for those by definition).

### Examining Objects

The `unitizer` prompt is very much like the R prompt in as much as it allows you to evaluate arbitrary expresions, including examining objects.  One useful thing to do is to check what objects are available to examine, which you can do with `ls`:

    unitizer> ls()
    $`objects in new test env:`
    [1] "res" "x"   "y"  

    $`objects in ref test env:`
    [1] "res" "x"   "y"  

    $`unitizer objects:`
    [1] ".new" ".ref"

    Use `ref(obj_name)` to access reference objects.
    `.new` / `.ref` for test value, `getTest(.new)` / `getTest(.ref)` for details.
    unitizer> 

When you use `ls` from the `unitizer` prompt you are not using `base::ls`, as is obvious from the output above.  We need this special version of `ls` because the `unitizer` environment is more complex than the typical console environment.  For example, the objects present during the evaluation of the new tests may not be the same as they were during the reference evaluation, so we must make both of them available to the user.  Buy default any objects you attempt to access at the `unitizer` prompt will be the new objects, but you can use the `ref` function to access the reference versions:

    unitizer> res
        intercept         slope           rsq 
    -3.541306e+11  7.012486e+09  9.688545e-01 
    attr(,"class")
    [1] "fastlm"
    unitizer> ref(res)
     intercept      slope        rsq 
    -17.170000   1.010000   0.938679 
    attr(,"class")
    [1] "fastlm"
    unitizer> 

In this case `res` and `ref(res)` match up exactly to `.new` and `.ref`, but that is just because we purposefully converted the assignment to `res` into a test by wrapping it in parentheses.  If you did have a divergence in an object that was created outside of a test, the only way to compare its new value to its reference value would be to use `ref`.

### Quitting `unitizer`

At any point you may quit `unitizer` by typing `Q` at the `unitizer` prompt.  If you have already reviewed tests, you will be given the opportunity to save what you have done so far:

    unitizer> Q
    = Finalize Unitizer =================================================

    No items to store; there either were no changes or you didn't accept any changes.
    Exit unitizer ([Y]es, [B]ack, [R]eview)?
    unitizer> Y
    unitizer store was not modified

Before `unitizer` quits completely we are given the chance to review are changes by using the `B` or `R` navigation commands.  In this case we just quit without further ado; since we had not chosen any of the new test values to replace the old there was no need to modify the unitizer store.

## Esoteric Topics
----

### Subtleties Of The Interactive Environment

Clearly the `unitizer` prompt environment is not as straightforward as the standard R console environment, or even the `browser` environment.  In reality it is even more complex than we've shown so far.

`unitizer` batch processes all the tests when it is first run before it breaks into interactive mode.  It does this for two reasons: 1. this allows it to display useful summary data (how many tests passed/failed in which sections), and more importantly, 2. it means a potentially time consuming process can be left to run unattended, and then the interactive portion of the test review is not interrupted by lengthy evaluations each time a user moves on to the next test.

While there are good reasons for batch processing the tests, it means that the review process is complicated substantially.  If we used a single evaluation environment it would be cluttered by all the objects created throughout the whole test script, so when we started reviewing the first test we would be seeing all the objects, even objects that did not exist when the first test was run.  Much worse, it is entirely possible for a variable to be re-used several times in a file, adopting different values for different tests, with the implication that objects used by our first test may have values that are different to what they were when the test was originally run.

To get around this problem each test is run in its own environment.  Each of these environments has for parent the environment of the previous test.  This means that a test has access to all the objects created/used by earlier tests, but not objects created/used by subsequent tests.  When a later test "modifies" an existing object, the existing object is not really modified; rather, the test creates a new object of the same name in the child environment which masks the object in the earlier test.  This is functionally equivalent to overwriting the object as far as the later test is concerned.

For the most part this environment trickery should be transparent to the user.  The `ls` function is specially modified to, among other things, list objects in all the parent test environments.  But there are exceptions to this "transparency".  The simplest exception is that you can't actually remove an object created in an earlier test (well, it is possible, but the how isn't documented and you are advised not to attempt to do it).  Here is a more complex exception:

    a <- function() b()
    b <- function() TRUE
    a()

In this case, when we evaluate `a()` we must step back two environments to find `a`, but that's okay.  The problem is that once inside `a`, we must now evaluate `b()`, but `b` is defined in a child environment, not a parent
environment so R's object lookup fails.

Now, the above example actually works because as noted in "What Constitutes a Test", environments are only defined for tests, an neither the `a` or `b` assignments are tests, so both `a` and `b` are assigned to the environment
of the `a()` call.  However, this really breaks:

    a <- function() b()
    NULL
    b <- function() TRUE
    a()

Since NULL is a valid test, `a` is assigned to the NULL test environment, and `b` is assigned to the `a()` call test environment, and the illusion is shattered.

If you are getting weird "object not found" errors when you run your tests, but the same code doesn't generate those errors when run directly in the command line, the illusion could be failing you.  In those situations, make sure that you assign all the variables necessary right ahead of the test so they will all get stored in the same environment.

### Patchwork Reference Environments

## Extreme Programming with `unitizer`
----

One way to approach unit testing is write the unit tests before you even develop the functionality.  This ensures that the tests are actually written, and it also serves as a documentation of sorts since presumably all aspects of functionality will have some explicit test assigned.  For example, an extreme programming file for the `times_two` function might look like so:

    ### file: tests/extreme/timestwo.R

    all.equal(c(2, 4, 6), times_two(1:3))
    all.equal(c(10, 20), times_two(c(5, 10)))
    tryCatch(
      times_two("hello"), 
      error=function(e) {
        if(!identical(conditionMessage(e), "input must be numeric"))
          stop("Failed Test")
        return(TRUE)
      }
    )

You then set-out to write `times_two` and keep running the file until every line evaluates to `TRUE`.  `unitizer` is not really designed for this programming philosophy, but you can still use it.  There are two options, either document and ensure that the tests are evaluating as expected on review (and reject if not):

    ### file: tests/unitizer/timestwo.R

    times_two(1:3)        # double vector values
    times_two(c(5, 10))   # same
    times_two("hello")    # error: input must be numeric

Or use the same file as the extreme programmer would and ensure that all tests evaluate to TRUE on review (and reject if not).

Neither of these are perfect substitutes as in both cases you have to review the tests which risks becoming tedious quickly.  In the future we will add an "extreme" mode to `unitizer` that would automatically reject tests that do not evaluate to TRUE.