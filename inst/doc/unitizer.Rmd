---
title: "unitizeR - Turn Arbitrary Code into Unit Tests"
output:
  html_document:
    toc: true
    theme: united
    css: unitizer.css
---

## Prologue

This is a very long vignette.  `unitizer` bakes in a lot of contextual help so that you don't have to read beyond the "Introduction" section of this vignette in order to use it.  If you reach a `unitizer>` prompt and don't know what to do, try typing `H` and hitting the "ENTER" key.

Your best bet to start unitizing quickly is to read the introduction to this vignette.  Here is a quick overview of what the vignette covers:

* "Introduction": Overview of the rationale for `unitizer`, and quick start guide
* "A Detailed Run Through": Walks step by step through a sample use of `unitizer`
* "Details On Tests": What constitutes a test, how to change what aspects of a test are tested
* "Test Capture": Details on how `unitizer` captures test values / conditions / etc.
* "The Interactive Environment": Detailed review of user options available at the `unitizer>` prompt
* "Esoteric Topics": The `unitizer` interactive environment is quite complex; this section gets into the nitty gritty details
* "Extreme Programming with `unitizer`": A discussing of "extreme programming" in the context of `unitizer`

## Introduction

### Storing Expressions and Their Results as Tests

One of the more important purposes of unit tests is to ensure that previously tested functionality does not break as a result of changes to the code base.  `unitizer` seeks to simplify this aspect of unit testing by taking advantage of the following:

* code functionality is demonstrated by evaluating expressions and observing results
* unit tests verify that specific expressions continue to produce the same results
* in R, expressions are objects, results are objects, and both can be stored

`unitizer` evaluates arbitrary expressions and stores them along with the result of their evaluation.  Re-running the same `unitizer` then compares the new results for each expression to the previously stored results.  If there are any differences `unitizer` breaks out into an interactive mode to allow comparison of the old evaluation to the new one, and if necessary, update the tests.

### But Why?

There are some note-worthy benefits to this approach, in particular:

 1. tests for expressions that produce complex objects (e.g. `lm(y ~ x + z), DF)`) are trivial to write because the result of the expression becomes the unit test
 2. there is no coding overhead to writing the tests; what you typed in the console when you were developing functionality can be used as the unit tests
 3. (almost) all aspects of the evaluation can be captured and used as part of the tests; for example, any conditions signalled during evaluation will be captured for comparison to future evaluations

We first embarked on this project to address point 1.  In the absence of `unitizer`, testing that complex objects are created as expected requires running the test expression, deparsing the result, and then copy/pasting the ungainly string into a file with an `all.equal` or equivalent such statement.  While this isn't back-breaking, it does start taking on sisyphean overtones when you set out to build comprehensive tests, or god forbid, realize you need to slightly modify the output of your functions.

Points 2. and 3. are gravy.  For example, for 3., you can easily test that your code both fails, and that when it does, it produces the desired error message.

One aspect of evaluation that isn't currently captured but hopefully will be in the future are side effects such as plots.

### How Do I Use It?

There are several ways to use `unitizer`, but the simplest one is to create a file with your test expressions.  For example, suppose we develop a `fastlm` function that does simple single variable regression to calculate slope, intercept, and coefficient.  Our test file might look like so:

    ## file: tests/unitizer/fastlm.R

    library(fastlm)

    x <- 1:100
    y <- ((1:100) / 10) ^ 2

    (res <- fastlm(x, y))   # parens to force treatment as test
    get_slope(res)
    get_intercept(res)
    get_rsq(res)

    # Cause errors

    fastlm(x, head(y))
    fastlm(x, NULL)

    get_slope("hello")
    get_rsq(FALSE)

Then, we just run:

    unitize("tests/unitizer/fastlm.R")

The expressions will be evaluated, and results, errors, warnings, screen ouput, etc. will be stored.  If this is the first time you've run `unitize`, you will be prompted to review each test in an interactive environment to verify the expressions actually produce the desired outcome.  Once that's done, everything is stored in an `RDS` and voila, unit tests! 

Later on, you can just re-run:

    unitize("tests/unitizer/fastlm.R")

If everything is working as before then all the tests will pass and you can keep coding merrily.  If you introduced regressions, then `unitizer` will highlight the failing tests and allow you to review them interactively.  If you added more tests to the file since the last time you ran `unitize`, those tests will be added to the RDS subject to your review.

### Learning To Use `unitizer`

You don't need to go through the entirity of this oxymoronically lengthy vignette to start using `unitizer`.  The `unitizer` interactive environment strives to be self explanatory, and provides contextual help.  You can always type `H` at the `unitizer` prompt to receive assitance.

### Non-Interactive Use

`unitizer` works fine in non-interactive environments, though in this case the only way for `unitizer` to succeed is if there are no changes to the test file or to the evaluation results.  If this is not the case, you will need to run `unitizer` in an interactive environment to identify the problems and either resolve them or update the `unitizer` store.

### `unitizer` and Packages

The simplest way to use `unitizer` as part of your package development process is to create a `tests/unitizer` folder for all your `unitizer` test scripts, and then, in `tests`, add an additional file with calls to `unitize`.  For example, in addition to our `tests/unitizer/fastlm.R` file, we would have:

    ## file: tests/run.R

    library(unitzer)
    unitize("unitizer/fastlm.R")

You can have multiple `unitize` statements.  Note that the path specification for test files should be relative to the `tests` directory as that is what `R CMD check` effectively sets the working directory to before running the files in `tests/`.  This means you can't just source your `run.R` file without also first setting the working directory to `tests/`.

## A Detailed Run-through

### Overview

In this section we will go through the process of creating a new set of tests, verifying they work as expected, modifying our code base, and then re-running `unitizer` to ensure we did not introduce regressions.  We will use the same test file we used in the "Introduction" section (`tests/unitizer/fastlm.R`).

### First Run - Store Tests

The first time we run our tests we are prompted to review them one by one to confirm they evaluated to what we expect.  This is what happens when we run `unitize("tests/unitizer/fastlm.R")`:

    > unitize("tests/unitizer/fastlm.R")
    +--------------------------------------------------------------------+
    | unitizer for: tests/unitizer/fastlm.R                              |
    +--------------------------------------------------------------------+

        Pass     Fail      New  Deleted                                   
           0        0        8        0 

    - New ----------------------------------------------------------------

    Test script contains tests not present in unitizer. Add new item to 
    store ([Y]es, [N]o, [B]ack, [R]eview, [Q]uit, [H]elp)?

    > library(fastlm)
    > x <- 1:100
    > y <- ((1:100)/10)^2
    # parens to force treatment as test
    > (res <- fastlm(x, y))
     intercept      slope        rsq 
    -17.170000   1.010000   0.938679 
    attr(,"class")
    [1] "fastlm"
    unitizer> 

This is telling us that there are eight new tests in our file, and the "new" banner indicates we are about to review the new tests (the only type in this case).  The text that follows provides options to type at the `unitizer>` prompt.

Following the banner, `unitizer` starts showing you the results of evaluating the test file and eventually stops after the first test with a `unitizer>` prompt.  Note that simple assignment expressions are not considered tests which is why the prompt only shows up after the `(res <- fastlm(x, y))` line (we forced this to be a test by wrapping it in parentheses).

As we noted earlier, the prompt allows you to run R expressions, so we can use this opportunity to check that our function did what it is supposed to:

    unitizer> lm(y ~ x, data.frame(x, y))$coefficients
    (Intercept)           x 
         -17.17        1.01 
    unitizer>

We visually compare the output of `lm` to `fastlm` and conclude they match, so we accept the test by typing in `Y` and hitting the "ENTER" key:

    unitizer> Y
    > get_slope(res)
    [1] 1.01
    unitizer>

The first test, `(res <- fastlm(x, y))` is now queued for storing, and we're prompted about the second test (`get_slope(res)`).  We will also accept this one, as well as the next 3, which brings us to:

    # Cause errors
    > fastlm(x, head(y))
    Error in data.frame(x = x, y = y) : 
      arguments imply differing number of rows: 100, 6
    unitizer> 

This test caused an error.  It was supposed to as the inputs were purposefully wrong.  We want to store this particular error to ensure that in the future our function keeps erroring out in the same way with this type of erroneous input.  Conveniently all we have to do is hit `Y`.  The error becomes part of the store and any future re-runs will compare the result of the expression against the stored error and will notify if the expression fails to produce an error or produces a different error.  So will we accept this test, as well as the remaining tests, which brings us to the final prompt:

    unitizer> Y
    = Confirm Changes ====================================================

    You are about to IRREVERSIBLY:
    - Add 8 out of 8 new tests
    unitizer>

So far we have only queued the tests for storage.  By answering `Y` at this prompt we will be storing all the tests we accepted into the unitizer store:

    unitizer> Y
    unitizer updated

That's it.  We're done for now.  On the actual R prompt:

    > list.files("tests/unitizer")
    # [1] "fastlm.R"   "fastlm.unitizer"

`fastlm.unitizer` is a folder that contains the `.rds` file(s) generated by unitizer.  These file(s) contain the data we will use in the future to ensure we have not introduced regressions in our code.

### Second Run -- Check For Regressions

Turns out our `fastlm` wasn't fast at all.  It was just a wrapper for the standard `lm` function.  But that was just a starting point.  After our initial run we went ahead and figured out all the formulas needed to calculate the slope, intercept and R squared of a single variable linear regression, and plugged them into our function.  But did we do it right?  Let's check:

    > unitize("tests/unitizer/fastlm.R")
    +-------------------------------------------------------------------+
    | unitizer for: tests/unitizer/fastlm.R                             |
    +-------------------------------------------------------------------+

        Pass     Fail      New  Deleted                                  
           2        6        0        0 

    - Failed ------------------------------------------------------------

    Reference test does not match new test from test script. Overwrite item in store with new value ([Y]es, [N]o, [B]ack, [R]eview, [Q]uit, [H]elp)?

    # parens to force treatment as test
    > (res <- fastlm(x, y))
    Test Failed Because:
    Value mismatch: Mean relative difference: 18889539312
    unitizer> 

Clearly something is wrong.  Six of our eight tests failed.  `unitizer` prompts us to review the failed tests and decide whether to replace the existing tests with the new ones.  The first failed test is the one that calculates the values related to the linear regression.  `unitizer` provides us with two special objects that we can use to examine our test failure: `.new` and `.ref`, each representing the value produced by the test we just ran as well as the value produced by the original test we stored.  Let's examine them:

    unitizer> .new
        intercept         slope           rsq 
    -3.541306e+11  7.012486e+09  9.688545e-01 
    attr(,"class")
    [1] "fastlm"
    unitizer> .ref
     intercept      slope        rsq 
    -17.170000   1.010000   0.938679 
    attr(,"class")
    [1] "fastlm"
    unitizer>  

By comparing the output of `.new` and `.ref` we can see that all our computations are just wrong (note we could also have re-evaluated our test instead of typing `.new` at the prompt, but if the computation is slow using `.new` avoids recomputation, and the only way to get the original value is with `.ref`).

We now know that our new calculations are wrong, so we will reject this test as well as the subsequent related tests (all the `get_*` functions that retrieve computations will obviously fail too), which takes us to our error causing tests:

    unitizer> N
    > get_intercept(res)
    Test Failed Because:
    Value mismatch: Mean relative difference: 5.941176
    unitizer> N
    # Cause errors
    > fastlm(x, head(y))
    Error in fastlm(x, head(y)) : 
      Arguments `x` and `y` must be the same length.
    Test Failed Because:
    Conditions mismatch: 
      + There is 1 condition mismatch; showing first mismatch at condition #1
      + Error condition messages do not match
    unitizer>

This is telling us that our tests failed because they did not produce the same error message.  The problem is that neither `.new` nor `.ref` are helpful at this point since we are interested in the error messages produced by the expressions, not the values.  Fortunately, `unitizer` defines `getConds`, which in combination with `.new`/`.ref` allows us to extract the conditions produced during test evaluation:

    getConds(.new)[[1]]
    <simpleError in fastlm(x, head(y)): Arguments `x` and `y` must be the same
    length.>
    unitizer> getConds(.ref)[[1]]
    <simpleError in data.frame(x = x, y = y): arguments imply differing number 
    of rows: 100, 6>

Note `getConds` returns a list of conditions, which is why we subset with `[[1]]` to see the first condition. By using the `.new`/`.ref` objects as an argument we can tell `getConds` which of the conditions we wish to view: the newly evaluated ones or the reference ones that were stored earlier.

In this case, while the condition messages changed, the general meaning did not and since we added our own error handler to replace internal error handling this change is reasonable. The remaining two failed tests also failed for similar reasons, so we will accept them too, which again takes us to the final prompt:

    unitizer> Y
    > fastlm(x, NULL)
    Error in fastlm(x, NULL) : Arguments `x` and `y` must be numeric.
    Test Failed Because:
    Conditions mismatch: 
      + There is 1 condition mismatch; showing first mismatch at condition #1
      + Error condition messages do not match
    unitizer> Y
    = Confirm Changes ===================================================

    You are about to IRREVERSIBLY:
    - Replace 2 out of 6 failed tests
    unitizer> Y
    unitizer updated

Our `unitizer` `.rds` is now updated.  We replaced the two error tests with their new versions, but did not replace the tests related to the actual computations since we know those new computations are wrong.

### Third Run -- Fix Problems

We figured out the problem in our new implementation of `fastlm` and fixed it.  Let's run unitizer one more time:

    > unitize("tests/unitizer/fastlm.R")
    +--------------------------------------------------------------------+
    | unitizer for: tests/unitizer/fastlm.R                              |
    +--------------------------------------------------------------------+

        Pass     Fail      New  Deleted                                   
           8        0        0        0 

    All tests passed; nothing to store.

Success! Our new test code now validates properly.  We have a functional `fastlm`!

### A Segue on Test Types

So far we have only seen three test types: New, Passed, and Failed.  For reference here is a list of all the test types:

* New: the test is not present in the `unitizer` store and needs to be reviewed prior to storage to ensure it actually does what you expect
* Passed: the test passed successfully and you will not be prompted to review
* Failed: the stored test differs from the one produced by the re-evaluation of the same expression, as was the case in our examples above
* Deleted/Removed: a test present in the `unitizer` store no longer exists in the test R file
* Corrupted/Error: an error occurred while attempting to compare the new and reference tests; this should occur very rarely and is likely the result of using a custom comparison function to compare the tests (see `unitizer_sect` section for more details on custom comparison functions).  When this happens `unitizer` has no way of knowing whether the test passed or failed; you can think of it as an `NA` outcome.

When reviewing tests, `unitizer` will group tests by test type, so you will review all new tests for each `unitizer_sect` in one go, then the failed tests, and so on.  The implication here is that the order that you review tests may not be the same as the order they appear in in the test file.

## Details On Tests

### What Constitutes a Test?

As noted previously simple assignments are not considered tests.  They are stored in the `unitizer` store, but you are not asked to review them, and their values are not be compared to values produced by future evaluations of the same assignment expression.  The implicit assumption is that if there is an assignment the intent is to use it as an input to another test that will be reviewed, so skipping assignment review saves some unnecessary user interaction.

You can force assignments to become tests by wrapping them in parentheses, which is what we did in our test file with `(res <- fastlm(x, y))`.

In addition to assignments, some expressions are not considered tests.  At this time, another expression that is ignored is `library(...)`, as the return value for that function varies depending on the currently loaded libraries.

### Test Sections: `unitizer_sect`

Often it is useful to group tests in sections for the sake of documentation and clarity.  Here we updated our original test file to add some sections:

    ## file: tests/unitizer/fastlm.R

    library(fastlm)

    unitizer_sect(
      "Base Tests", 
      {
        x <- 1:100
        y <- ((1:100) / 10) ^ 2

        (res <- fastlm(x, y))   # parens to force treatment as test
        get_slope(res)
        get_intercept(res)
        get_rsq(res)
    } )
    unitizer_sect(
      "Test Error Handling", 
      {
        fastlm(x, head(y))
        fastlm(x, NULL)

        get_slope("hello")
        get_rsq(FALSE)
    } )

Now re-running `unitizer` segments everything by section:

    > unitize("tests/unitizer/fastlm.R")
    +-------------------------------------------------------------------+
    | unitizer for: tests/unitizer/fastlm.R                             |
    +-------------------------------------------------------------------+

                        Pass     Fail      New  Deleted                  
    Base Tests             4        0        0        0
    Test Error H...        4        0        0        0
    **Total**              8        0        0        0

    All tests passed; nothing to store.

Similarly, if there are tests that require reviewing, each section will be reviewed in turn.

### `unitizer` Test Components

The following aspects of a unitizer tests are recorded for future comparison:

* Value (whether invisible or not)
* Conditions
* Screen output
* Message (stderr) output
* Whether the expression issued an "abort" `invokeRestart`

Currently only the first two elements are actually compared when determining whether a test passes or fails.  These two should capture almost all you would care about from a unit test perspective.  Screen output is omitted from comparison because it can be caused to vary substantially by factors unrelated to source code changes (e.g. console display width).  Message output is omitted because all typical mechanisms for producing stderr output also produce conditions with messages embedded, so it is usually superfluous to compare them.  The "abort" `invokeRestart` is omitted because it generally is implied by the presence of an error condition and actively monitoring it clutters the diagnostic messaging produced by `unitizer`.

While we omit the last three components from comparison, this is just default behavior.  You can change this by using the `compare` argument for `unitizer_sect` (see next section).

### Controlling Test Comparison

By default tested components (values and conditions) are compared with `all.equal`.  If you want to override the function used for value comparisons it is as simple as creating a new section for the tests you want to compare differently and use the `compare` argument:

     unitizer_sect(
      "Base Tests Strict", compare=identical,
      {
        get_slope(res)
        get_intercept(res)
        get_rsq(res)
    } )

The values produced by these three tests will be compared using `identical` instead of `all.equal`.  If you want to modify how other components of the test are compared, then you can pass a `unitizerItemTestsFuns` object as the value to the `compare` argument instead of a function:

     unitizer_sect(
      "Base Tests Strict", 
      compare=unitizerItemTestsFuns(
        value=identical, 
        output=all.equal,
        message=identical
      ),
      {
        get_slope(res)
        get_intercept(res)
        get_rsq(res)
    } )

This will cause the value of tests to be compared with `identical`, the screen output with `all.equal`, and messages (stdout) with `identical`.  If you want to change the comparison function for conditions, keep in mind that what you are comparing are lists of conditions.  `unitizer` by default uses the `all.equal.condition_list` functions to compare conditions, and you should probably review that function if you intend to use a custom condition comparison function.

### Matching Tests

Whenever you re-run `unitizer` on a file that has already been "unitized", `unitizer` matches the expressions in that file to what the expressions were in the file when it was first run.  `unitizer` matches only on the deparsed expression, and does not care at all where in the file the expression occurs.  If multiple identical expressions exist in a file they will be matched in the order they show up.

One particularly important point is that the `unitizer_sect` in which a test was when it was first "unitized" has no bearing whatsoever on matching.  In other words, the matching is done with no regard for what `unitizer_sect` the new or reference tests are/where in.

### Commenting Tests

`unitizer` parses the comments in the test files and attaches them to the test that they document.  Comments are attached to tests if they are on the same line as the test, or just above the test.  Comments are displayed with the test expression during the interactive review mode.

## Test Capture

### Options and Streams

In order to properly capture output, this function must mess with streams and options.  In particular, it will do the following:

* temporarily set `options(warn=1L)` during expression evaluation
* temporarily set `options(error=null)` during expression evaluation
* use `sink()` to capture any output to `stdout`
* use `sink(type="message")` to capture output to `stderr`

This should all be transparent to the user, unless the user is also attempting to modify these settings in the test expressions.  The problematic interaction are around the `options` function.  If the user sets `options(warn=1)` with the hopes that setting will persist beyond the execution of the test scripts, that will not happen.  If the user sets `options(error=recover)` or some such in a test expression, and that expression throws an error, you will be thrown into recovery mode with no visibility of `stderr` or `stdout`, which will make for pretty challenging debugging.

If the function is run with message diversion already activated, then it will not capture any messages produced by the test expressions.  If one of the test expressions enables message capture, then this function will stop capturing messages from that point on.  If after this another test expression disables message capture, this function will start capturing messages starting with the subsequent test expression (i.e. any message output between the expression self disabling message output and the end of the expression will go to `stdout`).

### Error Handling

Because `unitize` evaluates test expressions within a call to `withCallingHandlers`, there are some limitations on successfully running `unitize` inside your own error handling calls.  In particular, `unitize` will not work properly if run inside a `tryCatch` or `try` statement. If test expressions throw conditions, the internal 
`withCallingHandlers` will automatically hand over control to your `tryCatch`/`try` statement without an opportunity to complete `unitize` computations.  Unfortunately there does not seem to be a way around this since we have to use `withCallingHandlers` so that test statements after non-aborting conditions are run.

## The Interactive Environment

### Example Set-up

To examine the interactive environment more thoroughly we will go back to the test review when our new version of `fastlm` was returning incorrect values.  This is the fourth failed test:

    > get_rsq(res)
    Test Failed Because:
    Value mismatch: Mean relative difference: 0.03214676
    unitizer>

We will use this test to illustrate features / subtleties of `unitizer` prompt.

### `unitizer` Commands

Much like the `browser()` prompt, the `unitizer` prompt accepts several special expressions that allow you to control `unitizer` behavior.  What the expressions are and what they do depends on context.  We will review them in the context of the failed test described above.  Let's go back and look at what the `unitizer` prompt stated before we started reviewing our failed tests:

    - Failed ------------------------------------------------------------

    Reference test does not match new test from test script. Overwrite 
    item in store with new value ([Y]es, [N]o, [B]ack, [R]eview, [Q]uit, 
    [H]elp)?

This clearly lays out all the special commands available to us:

* `Y` will accept the new value as the correct reference value to use for a test
* `N` will keep the previous reference value as the reference value for future tests
* `B` takes us back to the previously reviewed test (see "Navigation" sub-section)
* `R` allows us to navigate to any previously reviewed test (see "Test Navigation" sub-section)
* `Q` quits `unitizer` (see "Quitting `unitizer`")
* `H` provides contextual help

If you type any of those letters (without backticks) into the `unitizer` prompt you will cause `unitizer` to respond as described above instead of evaluating the expression as it would be at the normal R console prompt.  If you have a variable assigned to one of those letters and you wish to access it, you can do so with any of `get`, `(`, `print`, etc.  For example, suppose we stored something in `Y`, then to access it all these commands would work:

* `(Y)`
* `get("Y")`
* `print(Y)`

`unitizer` checks for an exact match of a user expression to the special command letters, so something like `(Y)` does not match `Y` which allows you to reach the value stored in `Y`.

If at any time you forget what `unitizer` options are available to you you can just hit the "ENTER" key and `unitizer` will re-print the options to screen.

### Test Navigation

The `B` and `R` `unitizer` commands allow you to navigate through previously evaluted tests.  This is useful if you realize that you incorrectly accepted or rejected an earlier test, but don't wish to quit `unitizer` completely and lose all the other properly reviewed tests. `B` just steps you back to the previous test.  `R` gives you the option to go back to any previously reviewed test.  At this time it is not possible to skip ahead to unreviewed tests.

`B` is trivially straightforward, so we will not discuss it further.  We will type `R` at the prompt of our fourth failed test to examine what it does:

    unitizer> R
    What test do you wish to review (input an integer-like number corresponding to a test)?

      2. (res <- fastlm(x, y)) - Failed:N
      3. get_slope(res) -------- Failed:N
      4. get_intercept(res) ---- Failed:N
    unitizer>

`R` produces a list of all the test we have reviewed.  In this case these are all failed tests which we chose not to keep (hence the `Failed:N` appended at the end of each deparsed expression).  If we have second thoughts about our choice we can revisit the test by inputting the test number at the prompt (note that the test numbering is a bit odd; there are reasons for this but they aren't worth discussing).  Suppose we want to review test 3, the second on our list.  Then:

    unitizer> 3
    You are re-reviewing a test; previous selection was: "N"
    > get_slope(res)
    Test Failed Because:
    Value mismatch: Mean relative difference: 6943055624
    unitizer> 

`unitizer` tells us we are re-reviewing this test and that previously we had chosen not to keep the new version.  At this point we could re-examine the test, and potentially change our previous selection.

### `.new` and `.ref`

As we saw in our earlier example there are special objects available at the prompt: `.new`, and for all but new tests, `.ref`.  These objects contain the values produced by the newly evaluated test (`.new`) and by the test when it was previously run and accepted (`.ref`).  `.new` might seem a bit superfluous since the user can always re-evaluate the test expression at the `unitizer` prompt to review the value, but if that evaluation is slow you can save a little time by using `.new`.  `.ref` is the only option you have to see what the test used to produce back when it was first accepted into the `unitizer` store.

`.new` and `.ref` contain the values produced by the tests, but sometimes it is useful to access other aspects of the test evaluation.  To do so you can use the `get*` accessor functions in conjunction with the `.new` or `.ref` objects:

* `getTest(.new)` gets general information about the test
* `getVal(.new)` returns the test value; equivalent to typing `.new` at the prompt
* `getConds(.new)` returns the list of conditions produced by the test
* `getMsg(.new)` returns the stderr captured during test evaluation
* `getOut(.new)` returns the screen output captured during test evaluation
* `getExpr(.new)` returns the test expression (don't confuse this with `getCall`, a built-in R function with no relation to `unitizer`)
* `getAborted(.new)` returns whether the test expression invoked an "abort" restart

You can substitute `.ref` for `.new` in any of the above, provided that `.ref` is defined (i.e. that won't work when you are reviewing new tests since there is no corresponding reference test for those by definition).

### Examining Objects

The `unitizer` prompt is very much like the R prompt in as much as it allows you to evaluate arbitrary expresions, including examining objects.  One useful thing to do is to check what objects are available to examine, which you can do with `ls`:

    unitizer> ls()
    $`objects in new test env:`
    [1] "res" "x"   "y"  

    $`objects in ref test env:`
    [1] "res" "x"   "y"  

    $`unitizer objects:`
    [1] ".new" ".ref"

    Use `ref(obj_name)` to access reference objects.
    `.new` / `.ref` for test value, `getTest(.new)` / `getTest(.ref)` for details.
    unitizer> 

When you use `ls` from the `unitizer` prompt you are not using `base::ls`, as is obvious from the output above.  We need this special version of `ls` because the `unitizer` environment is more complex than the typical console environment.  For example, the objects present during the evaluation of the new tests may not be the same as they were during the reference evaluation, so we must make both of them available to the user.  Buy default any objects you attempt to access at the `unitizer` prompt will be the new objects, but you can use the `ref` function to access the reference versions:

    unitizer> res
        intercept         slope           rsq 
    -3.541306e+11  7.012486e+09  9.688545e-01 
    attr(,"class")
    [1] "fastlm"
    unitizer> ref(res)
     intercept      slope        rsq 
    -17.170000   1.010000   0.938679 
    attr(,"class")
    [1] "fastlm"
    unitizer> 

In this case `res` and `ref(res)` match up exactly to `.new` and `.ref` respectively, but that is just because we purposefully converted the assignment to `res` into a test by wrapping it in parentheses.  If you did have a divergence in an object that was created outside of a test, the only way to compare its new value to its reference value would be to use `ref`.

### Quitting `unitizer`

At any point you may quit `unitizer` by typing `Q` at the `unitizer` prompt.  If you have already reviewed tests, you will be given the opportunity to save what you have done so far:

    unitizer> Q
    = Finalize Unitizer =================================================

    No items to store; there either were no changes or you didn't accept any changes.
    Exit unitizer ([Y]es, [B]ack, [R]eview)?
    unitizer> Y
    unitizer store was not modified

Before `unitizer` quits completely we are given the chance to review are changes by using the `B` or `R` navigation commands.  In this case we just quit without further ado; since we had not chosen any of the new test values to replace the old there was no need to modify the unitizer store.

You can also use `quit()` from the `unitizer` prompt, though this is not recommended as it will override standard `unitizer` behavior.

## Esoteric Topics

### Subtleties Of The Interactive Environment

Clearly the `unitizer` prompt environment is not as straightforward as the standard R console environment, or even the `browser` environment.  In reality it is even more complex than we've shown so far.

`unitizer` batch processes all the tests when it is first run before it breaks into interactive mode.  It does this for two reasons: 1. this allows it to display useful summary data (how many tests passed/failed in which sections), and more importantly, 2. it means a potentially time consuming process can be left to run unattended, and then the interactive portion of the test review is not interrupted by lengthy evaluations each time a user moves on to the next test.

While there are good reasons for batch processing the tests, it means that the review process is complicated substantially.  If we used a single evaluation environment it would be cluttered by all the objects created throughout the whole test script, so when we started reviewing the first test we would be seeing all the objects, even objects that did not exist when the first test was run.  Much worse, it is entirely possible for a variable to be re-used several times in a file, adopting different values for different tests, with the implication that objects used by our first test may have values that are different to what they were when the test was originally run.

To get around this problem each test is run in its own environment.  Each of these environments has for parent the environment of the previous test.  This means that a test has access to all the objects created/used by earlier tests, but not objects created/used by subsequent tests.  When a later test "modifies" an existing object, the existing object is not really modified; rather, the test creates a new object of the same name in the child environment which masks the object in the earlier test.  This is functionally equivalent to overwriting the object as far as the later test is concerned.

For the most part this environment trickery should be transparent to the user.  The `ls` function is specially modified to, among other things, list objects in all the parent test environments.  But there are exceptions to this "transparency".  The simplest exception is that you can't actually remove an object created in an earlier test (well, it is possible, but the how isn't documented and you are advised not to attempt to do it).  Here is a more complex exception:

    a <- function() b()
    b <- function() TRUE
    a()

In this case, when we evaluate `a()` we must step back two environments to find `a`, but that's okay.  The problem is that once inside `a`, we must now evaluate `b()`, but `b` is defined in a child environment, not a parent
environment so R's object lookup fails.

Now, the above example actually works because as noted in "What Constitutes a Test", environments are only defined for tests, an neither the `a` or `b` assignments are tests, so both `a` and `b` are assigned to the environment
of the `a()` call.  However, this really breaks:

    a <- function() b()
    NULL
    b <- function() TRUE
    a()

Since NULL is a valid test, `a` is assigned to the environment associated with the `NULL` line, and `b` is assigned to the `a()` call test environment, and the illusion is shattered.

If you are getting weird "object not found" errors when you run your tests, but the same code doesn't generate those errors when run directly in the command line, the illusion could be failing you.  In those situations, make sure that you assign all the variables necessary right ahead of the test so they will all get stored in the same environment.

### Patchwork Reference Environments

When we review `unitizer` tests, it is possible to end up in a situation where we wish to update our store by keeping a mix of the new tests as well as some of the old ones.  This leads to some complications because in order to faithfully reproduce the environments associated with both the reference and the new tests we would potentially have to store the entire set of environments produced by the test script for both the new and reference tests.  Even worse, if we re-run unitizer again, we run the risk of having to store yet another set of environments (the old reference environments, what were new environments but became reference ones on this additional run, and the new environments created by this third run).  The problem continues to grow with as each incremental run of the `unitizer` script potentially creates the need to store yet another set of environments.

As a work-around to this problem `unitizer` only keeps the environment associated with the actual reference test when you type `N` at the `unitizer` prompt.  `unitizer` attaches then grafts that test to the environment set associated with the new test runs.  This means that in future `unitizer` runs the reference objects we examine when reviewing this test may not be the same objects that existed when the test itself was evaluated.  The `ls` command will highlight which objects have this problem.

Clearly this is not an ideal outcome, but the compromise was necessary to avoid the possibility of ever increasing `unitizer` stores.  For more details see the documentation for `unitizer:::healEnvs`.

### Alternate Store Locations

While this is not tested, in theory `unitizer` allows you to keep the `unitizer` store outside of the file system.  For example, you could keep the store in an SQL database.  To do so you need to define `get_store` and `set_store` S3 methods.  See the documentation for those functions for details on how to implement this.

### Overridden Functionality

In order to perpetuate the R console prompt illusion, `unitizer` needs to override some buit-in functionality, including:

* `ls` is replaced by a special version that can explore the `unitizerItem` environments
* `quit` and `q` are wrappers around the base functions that allow `unitizer` to quit gracefully
* `traceback` while not replaced, is supported by a fair bit of voodoo involving among other things assigning to `.Traceback`
* History is replaced during `unitizer` prompt evaluations with a temporary version of the history file containing only commands evaluated at the `unitizer` prompt.  The normal history file is restored on exit. 

## Extreme Programming with `unitizer`

One way to approach unit testing is write the unit tests before you even develop the functionality.  This ensures that the tests are actually written, and it also serves as a documentation of sorts since presumably all aspects of functionality will have some explicit test assigned.  For example, an extreme programming file the second through fourth tests in our `fastlm` test file might look like so:

    ### file: tests/extreme/fastlm.extreme.R

    # ... content omitted ...

    if(!all.equal(get_slope(res), 1.01)) stop("Test Failed") else TRUE
    if(!all.equal(get_intercept(res), -17.17)) stop("Test Failed") else TRUE
    if(!all.equal(get_rsq(res), 0.938679)) stop("Test Failed") else TRUE

    # ... content omitted ...

You then set-out to write the `fastlm` functions until every line evaluates to `TRUE`.  `unitizer` is not really designed for this programming philosophy, but there are ways to use it this way.  You can either document and ensure that the tests are evaluating as expected on review (and reject if not):

    ### file: tests/extreme/fastlm.extreme.unitizer.R

    # ... content omitted ...

    get_slope(res)        # should be 1.01
    get_intercept(res)    # should be -17.17
    get_rsq(res)          # should be 0.938679

    # ... content omitted ...

Or use the same file as the extreme programmer would and ensure that all tests evaluate to TRUE on review (and reject if not).

Neither of these are perfect substitutes as in both cases you have to review the tests to ensure the correct outcome, which risks becoming tedious quickly when you are developing the code and re-running the tests over and over to check for success.  If you are an active partitioner of extreme programming you may be better served by unit testing packages such as `testthat` (side note: you can actually combine `testthat` and `unitizer`; for example, for `fastlm.extreme.R`, you could use `expect_equal(get_slope(res), 1.01)` instead of `if(!all.equal(get_slope(res), 1.01)) stop("Test Failed") else TRUE`).

In our experience, trying to pre-define all functionality ahead of time in tests ends up being counter productive because much of what the functionality should be only becomes clear with testing, and more importantly, use.  We strongly believe that the the less a developer uses her programs, the worse those programs tend to be.  The implicit corollary is that it is very difficult to fully define good functionality and interfaces for programs before the program is actually used, which in turn implies that there will be a fair bit of test re-writing required along the way.  `unitizer` makes this type of iterative development very easy because the test themselves are informal, and re-writing these informal tests is easier than re-writing formal ones.

This doesn't mean that effort shouldn't be applied to define functionality and interfaces ahead of time, or that situations where they can or should be strictly defined don't exist.  For those formal unit tests are likely best.  For everything else, you can use `unitizer`.