---
title: "unitizeR - Esoteric Topics"
author: "Brodie Gaslam"
output:
    rmarkdown::html_vignette:
        toc: true
        css: styles.css

vignette: >
  %\VignetteIndexEntry{06 - Esoteric Topics}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

## Subtleties Of The Interactive Environment

Clearly the `unitizer` prompt environment is not as straightforward as the standard R console environment, or even the `browser` environment.  In reality it is even more complex than we have shown so far.

`unitizer` batch processes all the tests when it is first run before it breaks into interactive mode.  It does this for two reasons:

1. this allows it to display useful summary data (how many tests passed/failed in which sections), and more importantly,
2. it means a potentially time consuming process can be left to run unattended, and then the interactive portion of the test review is not interrupted by lengthy evaluations each time a user moves on to the next test.

While there are good reasons for batch processing the tests, it means that the review process is complicated substantially.  If we used a single evaluation environment it would be cluttered by all the objects created throughout the whole test script, so when we started reviewing the first test we would be seeing all the objects, even objects that did not exist when the first test was run.  Much worse, it is entirely possible for a symbol to be re-used several times in a file, adopting different values for different tests, with the implication that objects used by our first test may have values that are different to what they were when the test was originally run.

To get around this problem each test is run in its own environment.  Each of these environments has for parent the environment of the previous test.  This means that a test has access to all the objects created/used by earlier tests, but not objects created/used by subsequent tests.  When a later test "modifies" an existing object, the existing object is not really modified; rather, the test creates a new object of the same name in the child environment which masks the object in the earlier test.  This is functionally equivalent to overwriting the object as far as the later test is concerned.

For the most part this environment trickery should be transparent to the user.  The `ls` function is specially modified to, among other things, list objects in all the parent test environments.  But there are exceptions to this "transparency".  The simplest exception is that you can not actually remove an object created in an earlier test (well, it is possible, but the how isn't documented and you are advised not to attempt it).  Here is a more complex exception:

    a <- function() b()
    b <- function() TRUE
    a()

In this case, when we evaluate `a()` we must step back two environments to find `a`, but that's okay.  The problem is that once inside `a`, we must now evaluate `b()`, but `b` is defined in a child environment, not a parent environment so R's object lookup fails.

It turns out the above example actually works because as noted in [details on tests vignette](vgn03tests.html), environments are only defined for tests, an neither the `a` or `b` assignments are tests, so both `a` and `b` are assigned to the environment of the `a()` call.  However, this really breaks:

    a <- function() b()
    NULL
    b <- function() TRUE
    a()

Since NULL is a valid test, `a` is assigned to the environment associated with the `NULL` line, and `b` is assigned to the `a()` test environment, and the illusion is shattered.

If you are getting weird "object not found" errors when you run your tests, but the same code does not generate those errors when run directly in the command line, this illusion could be failing you.  In those situations, make sure that you assign all the variables necessary right ahead of the test so they will all get stored in the same environment.

One other point to keep in mind is that the `unitizer` prompt environment is different for each test you review, so if you assign any variables at the prompt, they will not be available at subsequent prompts.

## Issues With Reference Objects

For the most part R is a copy-on-modify language, which allows us to employ the trickery described above.  There are however "reference" objects that are not copied when they are modified.  Notable examples include environments, reference classes, and `data.table`.  Since our trickery requires us to keep copies of each object in different environments as they are modified, it does not work with reference objects since they are not automatically duplicated.

The main consequence of this is that when you are reviewing a test that involves a reference object, the value of that reference object during review will be the value after the last reference modification, which may have been made after the test you are reviewing.  The tests will still work as they should, passing if you did not introduce regressions, and failing otherwise.  However if you review a failed test you may have a hard time making sense of what happened since the objects you review will may not have the values they had when the test was actually run.

## Patchwork Reference Environments

When we review `unitizer` tests, it is possible to end up in a situation where we wish to update our store by keeping a mix of the new tests as well as some of the old ones.  This leads to some complications because in order to faithfully reproduce the environments associated with both the reference and the new tests we would potentially have to store the entire set of environments produced by the test script for both the new and reference tests.  Even worse, if we re-run unitizer again, we run the risk of having to store yet another set of environments (the old reference environments, what were new environments but became reference ones on this additional run, and the new environments created by this third run).  The problem continues to grow with as each incremental run of the `unitizer` script potentially creates the need to store yet another set of environments.

As a work-around to this problem `unitizer` only keeps the environment associated with the actual reference tests you chose to keep (e.g. when you type `N` at the `unitizer` prompt when reviewing a failed test).  `unitizer` then grafts that test and its environment to the environment chain from the newly evaluated tests (note that for all tests that pass, we keep the new version of the tests, not the reference one).  This means that in future `unitizer` runs where you examine this same reference test, the other "reference" objects available for inspection may not be from the same evaluation that produced the test.  The `ls` command will highlight which objects are from the same evaluation vs which ones are not (see the [discussion on `ls`](vgn04interactiveenvironment.html#ls)).

Clearly this is not an ideal outcome, but the compromise was necessary to avoid the possibility of ever increasing `unitizer` stores.  For more details see `?"healEnvs,unitizerItems,unitizer-method"`.

## Overridden Functionality

In order to perpetuate the R console prompt illusion, `unitizer` needs to override some buit-in functionality, including:

* `ls` is replaced by a special version that can explore the `unitizerItem` environments
* `quit` and `q` are wrappers around the base functions that allow `unitizer` to quit gracefully
* `traceback` while not replaced, is supported by a fair bit of voodoo involving among other things assigning to `base::.Traceback`
* History is replaced during `unitizer` prompt evaluations with a temporary version of the history file containing only commands evaluated at the `unitizer` prompt.  The normal history file is restored on exit.

## Storing `unitize`d Tests

### `unitizer` Writes to Your File System

#### Default Mode is to Store Tests in `rds` Files

`unitizer` stores unit tests and their results.  By default, it stores them in `rds` files in your filesystem.  You will be prompted before a file is saved to your filesystem.

 The `rds` file is placed in a directory with the same name as your test file, but with "unitizer" appended.  For example, if your tests are in "my_file_name.R", then `unitizer` will create a folder called "my_file_name.unitizer/" and put an `rds` file in it.

 See `?get_unitizer` for potential alternatives to saving to your file system.

#### File Space Considerations

If your tests produce massive objects, the `unitizer` `rds` file will be massive.  Try designing your tests so they will produce the smallest representative data structures needed for your tests to be useful.

Additionally, note that the `rds` files are binary, which needs to be accounted for when using them in [version controlled projects]().

#### Backup Your `unitizer` Stores

`unitizer` does not backup the `rds` beyond the single copy in the aforementioned folder.  Unit tests are valuable, and without the `rds` file `unitizer` tests become a lot less useful.  To the extent you backup your R test files, you should also backup the corresponding ".unitizer/" folder.  You could lose / corrupt your `unitizer` store in many ways.  Some non-exhaustive examples:

- Standard file system SNAFU
- Careless updates to existing `unitizer`
- `unitizer` developer accidentally introduces a bug that destroys your `unitizer`

Backup your `unitizer` stores!

#### Alternate Store Locations

`unitize` stores and loads `unitizer`s using the `set_unitizer` and `get_unitizer` S3 generics .  This means you can implement your own S3 methods for those generics to store the `unitizer` object off-filesystem (e.g. MySQL databse, etc).  See `?get_unitizer` for more details, though note this feature is untested.

If you only wish to save your `unitizer` to a different location in your filesystem than the default, you do not need to resort to these methods as you can provide the target directory with `unitize(..., store.id=)`.

### Version Control and Unitizer

#### Handling Binary Files

The main issue with using `unitizer` with a version controlled package is that you have to decide whether you want to include the binary `rds` files in the version control history.  Some options:

* do not track the binary files at all (but they are valuable and now not backed up)
* do not track the binary files at all, but implement a secondary back-up system (this sounds really annoying)
* Use a backed-up, non-file system store (see "Alternate Store Locations" above)
* Track the binary files, but only commit them for major releases

#### Suggested Workflow with Git

The approach we suggest is the last one above: just commit your `rds` files for major releases.  Also, note that to the extent you have a lot of test files that aren't changing, this should minimize how much binary data you commit.  One possible way to handle this with Git is to use the following command (adapted from twalberg's [SO answer](http://stackoverflow.com/questions/12288212/git-update-index-assume-unchanged-on-directory)).

**Warning**: be careful when you reset your working directory as your `rds` files will get reset to the last version that you committed (i.e. `git checkout -- .` will replace your currend `rds` with the one in the repo).  If you update your `rds` within a branch and then merge the branch back, make sure you either commit or stash the `rds` before the merge.

To implement:

```
git ls-files *.rds -z | xargs -0 git update-index --assume-unchanged
```
This will prevent the `rds` files as showing up as being modified when you run `git status`.  Just make sure you still commit your `rds` at key release points so you have backups of them.  Once you reach the major release point, you can:
```
git ls-files *.rds -z | xargs -0 git update-index --no-assume-unchanged
```

If you `--assume-unchanged` other files such as `.o` or `.so` files, you may need to remove those after a checkout as well so that `devtools::install` will recreate them from current sources.

## Error Handling

Because `unitize` evaluates test expressions within a call to `withCallingHandlers`, there are some limitations on successfully running `unitize` inside your own error handling calls.  In particular, `unitize` will not work properly if run inside a `tryCatch` or `try` statement. If test expressions throw conditions, the internal `withCallingHandlers` will automatically hand over control to your `tryCatch`/`try` statement without an opportunity to complete `unitize` computations.  Unfortunately there does not seem to be a way around this since we have to use `withCallingHandlers` so that test statements after non-aborting conditions are run.

## Other Issues

### Ancillary `unitizer` Packages

`unitizer` uses anscillary packages in examples and tests.  These are are in `inst/example.pkgs` in the `unitizer` sources.  You should not have a need to use these, but if you do:
```
unitizer.path <- system.file(package="unitizer")
library(devtools)
install(paste0(unitizer.path, "/example.pkgs/unitizerdummypkg1"))  # one of several dummy packages
```
## Modifying an Existing Unitizer

### `review`

`review` allows you to review all tests in a unitizer rds with the option of dropping tests from it.  See `?review`.

### `edit_fun_names`

*Warning*: this is experimental.

`edit_fun_names` allows you to modify function names in calls stored in an `unitizer`.  This is useful when you decide to change a function name, but otherwise leave the behavior and rest of the call unchanged.  You can then upate your test script and the renamed calls will be matched against the correct values in the `unitizer` store.  Without this you would have to re-review and re-store every test since `unitizer` identifies tests by the deparsed expression.

### `split`

There is currently no direct way to split a `unitizer` into pieces (see [issue #44](https://github.com/brodieG/unitizer/issues/44)), but the current work around is to:

1. copy the test file and the corresponding `unitizer` to a new location
2. edit the original test file to remove the tests we want to split off
3. run unitizer and agree to drop all removed tests (hint, use CAPSLOCK)
4. edit the new test file and remove the tests that are still in the old test file
5. run unitizer and agree to drop all removed tests

The net result will be two new `unitizer`, each with a portion of the tests from the original `unitizer`.  Clearly less than ideal, but will work in a pinch.

## Troubleshooting

### `unitizer` Freezes and Pops up "Selection:"

This is almost certainly a result of an R crash.  Unfortunately the normal mechanisms to restore `stderr` don't seem to work completely with full R crashes, so when you see things like:
```
+------------------------------------------------------------------------------+
| unitizer for: tests/unitizer/alike.R                                         |
+------------------------------------------------------------------------------+

Running: alike(data.frame(a = integer(), b = factor()), data.frame(a = 1:3, Selection:
```
what you are not seeing is:
```
 *** caught segfault ***
address 0x7fdc20000010, cause 'memory not mapped'

Traceback:
 1: .Call(ALIKEC_alike, target, current, int.mode, int.tol, attr.mode)
 2: alike(data.frame(a = factor(), b = factor()), data.frame(a = 1:3,     b = letters[1:3]))

Possible actions:
1: abort (with core dump, if enabled)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace
```
The "Selection:" bit is prompting you to type 1-4 as per above.  We will investigate to see if there is a way to address this problem, but the solution likely is not simple since the R crash circumvents the `on.exit` handlers used to reset the stream redirects.  Also, note that in this case the crash is caused by `alike`, not `unitizer` (see below).

### Running `unitizer` Crashes R

Every R crash we have discovered while using `unitizer` was eventually traced to a third party package.  Some of the crashes were linked to issues attaching/detaching packages.  If you think you might be having an issue with this you can always run with `clean.search.path=FALSE` and `clean.env=FALSE` to avoid search path manipulation.

### Different Outcomes in Interactive vs. Non Interactive

Watch out for functions that have default arguments of the type:
```
fun <- function(x, y=getOption('blahblah'))
```
as those options may be different depending on whether you are running whether you are running R interactively or not.  One prime example is `parse(..., keep.source = getOption("keep.source"))`.

